%
% Copyright © 2012 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%

%
%
%\input{../peeter_prologue.tex}

\chapter{Bivector grades of the squared angular momentum operator}
\index{angular momentum operator!bivector grades}
\label{chap:bivectorSelect}

%\blogpage{http://sites.google.com/site/peeterjoot/math2009/bivectorSelect.pdf}
%\date{Sept 6, 2009}
%\revisionInfo{\(RCSfile: bivectorSelect.tex,v \) Last \(Revision: 1.3 \) \(Date: 2009/10/22 02:07:20 \)}

%\beginArtWithToc
\beginArtNoToc

\section{Motivation}

The aim here is to extract the bivector grades of the squared angular momentum operator

\begin{equation}\label{eqn:bivectorSelect:goo1}
\begin{aligned}
\gpgradetwo{ (x \wedge \grad)^2 } \questionEquals \cdots
\end{aligned}
\end{equation}

I had tried this before and believe gotten it wrong.  Take it super slow and dumb and careful.

\section{Non-operator expansion}

Suppose \(P\) is a bivector, \(P = (\gamma^k \wedge \gamma^m) P_{km}\), the grade two product with a different unit bivector is

\begin{equation}\label{eqn:bivectorSelect:33}
\begin{aligned}
&\gpgradetwo{ (\gamma_a \wedge \gamma_b) (\gamma^k \wedge \gamma^m) } P_{km} \\
&=
\gpgradetwo{ (\gamma_a \gamma_b - \gamma_a \cdot \gamma_b) (\gamma^k \wedge \gamma^m) } P_{km} \\
&=
\gpgradetwo{ \gamma_a (\gamma_b \cdot (\gamma^k \wedge \gamma^m)) } P_{km}
+ \gpgradetwo{ \gamma_a (\gamma_b \wedge (\gamma^k \wedge \gamma^m)) } P_{km}
- (\gamma_a \cdot \gamma_b) (\gamma^k \wedge \gamma^m) P_{km} \\
&=
(\gamma_a \wedge \gamma^m) P_{b m} -(\gamma_a \wedge \gamma^k) P_{k b} - (\gamma_a \cdot \gamma_b) (\gamma^k \wedge \gamma^m) P_{km} \\
&+ (\gamma_a \cdot \gamma_b) (\gamma^k \wedge \gamma^m) P_{km}
- (\gamma_b \wedge \gamma^m) P_{a m}
+ (\gamma_b \wedge \gamma^k) P_{k a}
\\
&=
(\gamma_a \wedge \gamma^c) (P_{b c} -P_{c b})
+ (\gamma_b \wedge \gamma^c) (P_{c a} -P_{a c} )
\\
\end{aligned}
\end{equation}

This same procedure will be used for the operator square, but we have the complexity of having the second angular momentum operator change the first bivector result.

\section{Operator expansion}

In the first few lines of the bivector product expansion above, a blind replacement \(\gamma_a \rightarrow x\), and \(\gamma_b \rightarrow \grad\) gives us

\begin{equation}\label{eqn:bivectorSelect:53}
\begin{aligned}
&\gpgradetwo{ (x \wedge \grad) (\gamma^k \wedge \gamma^m) } P_{km}  \\
&=
\gpgradetwo{ (x \grad - x \cdot \grad) (\gamma^k \wedge \gamma^m) } P_{km} \\
&=
\gpgradetwo{ x (\grad \cdot (\gamma^k \wedge \gamma^m)) } P_{km}
+ \gpgradetwo{ x (\grad \wedge (\gamma^k \wedge \gamma^m)) } P_{km}
- (x \cdot \grad) (\gamma^k \wedge \gamma^m) P_{km} \\
\end{aligned}
\end{equation}

Using \(P_{km} = x_k \partial_m\), eliminating the coordinate expansion we have an intermediate result that gets us partway to the desired result

\begin{equation}\label{eqn:bivectorSelect:goo2}
\begin{aligned}
\gpgradetwo{ (x \wedge \grad)^2 }
&=
\gpgradetwo{ x (\grad \cdot (x \wedge \grad)) }
+ \gpgradetwo{ x (\grad \wedge (x \wedge \grad)) }
- (x \cdot \grad) (x \wedge \grad)
\end{aligned}
\end{equation}

An expansion of the first term should be easier than the second.  Dropping back to coordinates we have

\begin{equation}\label{eqn:bivectorSelect:73}
\begin{aligned}
\gpgradetwo{ x (\grad \cdot (x \wedge \grad)) }
&=
\gpgradetwo{ x (\grad \cdot (\gamma^k \wedge \gamma^m)) } x_k \partial_m \\
&=
\gpgradetwo{ x (\gamma_a \partial^a \cdot (\gamma^k \wedge \gamma^m)) } x_k \partial_m \\
&=
\gpgradetwo{ x \gamma^m \partial^k } x_k \partial_m
-\gpgradetwo{ x \gamma^k \partial^m } x_k \partial_m  \\
&=
x \wedge (\partial^k x_k \gamma^m \partial_m )
- x \wedge (\partial^m \gamma^k x_k \partial_m ) \\
\end{aligned}
\end{equation}

Okay, a bit closer.  Backpedaling with the reinsertion of the complete vector quantities we have

\begin{equation}\label{eqn:bivectorSelect:goo3}
\begin{aligned}
\gpgradetwo{ x (\grad \cdot (x \wedge \grad)) } &= x \wedge (\partial^k x_k \grad ) - x \wedge (\partial^m x \partial_m )
\end{aligned}
\end{equation}

Expanding out these two will be conceptually easier if the functional operation is made explicit.  For the first

\begin{equation}\label{eqn:bivectorSelect:93}
\begin{aligned}
x \wedge (\partial^k x_k \grad ) \phi
&=
x \wedge x_k \partial^k (\grad \phi)
+x \wedge ((\partial^k x_k) \grad) \phi \\
&=
x \wedge ((x \cdot \grad) (\grad \phi))
+ n (x \wedge \grad) \phi
\end{aligned}
\end{equation}

In operator form this is

\begin{equation}\label{eqn:bivectorSelect:goo4}
\begin{aligned}
x \wedge (\partial^k x_k \grad ) &= n (x \wedge \grad) + x \wedge ((x \cdot \grad) \grad )
\end{aligned}
\end{equation}

Now consider the second half of \eqnref{eqn:bivectorSelect:goo3}.  For that we expand

\begin{equation}\label{eqn:bivectorSelect:113}
\begin{aligned}
x \wedge (\partial^m x \partial_m ) \phi
&=
x \wedge (x \partial_m \partial^m \phi)
+ x \wedge ((\partial^m x) \partial_m \phi)
\end{aligned}
\end{equation}

Since \(x \wedge x = 0\), and \(\partial^m x = \partial^m x_k \gamma^k = \gamma^m\), we have

\begin{equation}\label{eqn:bivectorSelect:133}
\begin{aligned}
x \wedge (\partial^m x \partial_m ) \phi
&=
x \wedge (\gamma^m \partial_m ) \phi \\
&=
(x \wedge \grad) \phi
\end{aligned}
\end{equation}

Putting things back together we have for \eqnref{eqn:bivectorSelect:goo3}

%\gpgradetwo{ x (\grad \cdot (x \wedge \grad)) } &= x \wedge (\partial^k x_k \grad ) - x \wedge (\partial^m x \partial_m )
%x \wedge (\partial^k x_k \grad ) &= n (x \wedge \grad) + x \wedge ((x \cdot \grad) \grad )
%x \wedge (\partial^m x \partial_m ) \phi &= (x \wedge \grad) \phi

\begin{equation}\label{eqn:bivectorSelect:goo5}
\begin{aligned}
\gpgradetwo{ x (\grad \cdot (x \wedge \grad)) } &= (n-1) (x \wedge \grad) + x \wedge ((x \cdot \grad) \grad )
\end{aligned}
\end{equation}

This now completes a fair amount of the bivector selection, and a substitution back into \eqnref{eqn:bivectorSelect:goo2} yields

\begin{equation}\label{eqn:bivectorSelect:goo6}
\begin{aligned}
\gpgradetwo{ (x \wedge \grad)^2 }
&=
(n-1 - x \cdot \grad) (x \wedge \grad) + x \wedge ((x \cdot \grad) \grad )
+ x \cdot (\grad \wedge (x \wedge \grad))
\end{aligned}
\end{equation}

The remaining task is to explicitly expand the last vector-trivector dot product.  To do that we use the basic alternation expansion identity

\begin{equation}\label{eqn:bivectorSelect:goo7}
\begin{aligned}
a \cdot (b \wedge c \wedge d)
&=
(a \cdot b) (c \wedge d)
-(a \cdot c) (b \wedge d)
+(a \cdot d) (b \wedge c)
\end{aligned}
\end{equation}

To see how to apply this to the operator case lets write that explicitly but temporarily in coordinates

\begin{equation}\label{eqn:bivectorSelect:153}
\begin{aligned}
x \cdot ((\grad \wedge (x \wedge \grad)) \phi
&=
(x^\mu \gamma_\mu) \cdot ((\gamma^\nu \partial_\nu ) \wedge (x_\alpha \gamma^\alpha \wedge (\gamma^\beta \partial_\beta))) \phi \\
&=
x \cdot \grad (x \wedge \grad) \phi
-
x \cdot \gamma^\alpha \grad \wedge x_\alpha \grad \phi
+
x^\mu \grad \wedge x \gamma_\mu \cdot \gamma^\beta \partial_\beta  \phi \\
&=
x \cdot \grad (x \wedge \grad) \phi
-
x^\alpha \grad \wedge x_\alpha \grad \phi
+
x^\mu \grad \wedge x \partial_\mu  \phi
\end{aligned}
\end{equation}

Considering this term by term starting with the second one we have

\begin{equation}\label{eqn:bivectorSelect:173}
\begin{aligned}
x^\alpha \grad \wedge x_\alpha \grad \phi
&=
x_\alpha (\gamma^\mu \partial_\mu) \wedge x^\alpha \grad \phi \\
&=
x_\alpha \gamma^\mu \wedge (\partial_\mu x^\alpha) \grad \phi
+x_\alpha \gamma^\mu \wedge x^\alpha \partial_\mu \grad \phi  \\
&=
x_\mu \gamma^\mu \wedge \grad \phi
+x_\alpha x^\alpha \gamma^\mu \wedge \partial_\mu \grad \phi  \\
&=
x \wedge \grad \phi
+x^2 \grad \wedge \grad \phi  \\
\end{aligned}
\end{equation}

The curl of a gradient is zero, since summing over an product of antisymmetric and symmetric indices \(\gamma^\mu \wedge \gamma^\nu \partial_{\mu\nu}\) is zero.  Only one term remains to evaluate in the vector-trivector dot product now

\begin{equation}\label{eqn:bivectorSelect:goo8}
\begin{aligned}
x \cdot (\grad \wedge x \wedge \grad)
&=
(-1 + x \cdot \grad )(x \wedge \grad)
+
x^\mu \grad \wedge x \partial_\mu
\end{aligned}
\end{equation}

Again, a completely dumb and brute force expansion of this is

\begin{equation}\label{eqn:bivectorSelect:193}
\begin{aligned}
x^\mu \grad \wedge x \partial_\mu \phi
&=
x^\mu (\gamma^\nu \partial_\nu) \wedge (x^\alpha \gamma_\alpha) \partial_\mu \phi \\
&=
x^\mu \gamma^\nu \wedge (\partial_\nu (x^\alpha \gamma_\alpha)) \partial_\mu \phi
+x^\mu \gamma^\nu \wedge (x^\alpha \gamma_\alpha) \partial_\nu \partial_\mu \phi \\
&=
x^\mu (\gamma^\alpha \wedge \gamma_\alpha) \partial_\mu \phi
+x^\mu \gamma^\nu \wedge x \partial_\nu \partial_\mu \phi \\
\end{aligned}
\end{equation}

With \(\gamma^\mu = \pm \gamma_\mu\), the wedge in the first term is zero, leaving

\begin{equation}\label{eqn:bivectorSelect:213}
\begin{aligned}
x^\mu \grad \wedge x \partial_\mu \phi
&=
-x^\mu x \wedge \gamma^\nu \partial_\nu \partial_\mu \phi \\
&=
-x^\mu x \wedge \gamma^\nu \partial_\mu \partial_\nu \phi \\
&=
-x \wedge x^\mu \partial_\mu \gamma^\nu \partial_\nu \phi \\
\end{aligned}
\end{equation}

In vector form we have finally

\begin{equation}\label{eqn:bivectorSelect:goo9}
\begin{aligned}
x^\mu \grad \wedge x \partial_\mu \phi &= -x \wedge (x \cdot \grad) \grad \phi
\end{aligned}
\end{equation}

The final expansion of the vector-trivector dot product is now

\begin{equation}\label{eqn:bivectorSelect:goo10}
\begin{aligned}
x \cdot (\grad \wedge x \wedge \grad)
&=
(-1 + x \cdot \grad )(x \wedge \grad)
-x \wedge (x \cdot \grad) \grad \phi
\end{aligned}
\end{equation}

This was the last piece we needed for the bivector grade selection.  Incorporating this into \eqnref{eqn:bivectorSelect:goo6}, both the \(x \cdot \grad x \wedge \grad\), and the \(x \wedge (x \cdot \grad) \grad\) terms cancel leaving the surprising simple result

\begin{equation}\label{eqn:bivectorSelect:goo11}
\begin{aligned}
\gpgradetwo{ (x \wedge \grad)^2 }
%&=
%(n-1 - x \cdot \grad) (x \wedge \grad) + x \wedge ((x \cdot \grad) \grad )
%%+ x \cdot (\grad \wedge (x \wedge \grad))
%+(-1 + x \cdot \grad )(x \wedge \grad)
%-x \wedge (x \cdot \grad) \grad \phi
&=
(n-2) (x \wedge \grad)
\end{aligned}
\end{equation}

The power of this result is that it allows us to write the scalar angular momentum operator from the Laplacian as

\begin{equation}\label{eqn:bivectorSelect:233}
\begin{aligned}
\gpgradezero{ (x \wedge \grad)^2 }
&= (x \wedge \grad)^2 - \gpgradetwo{ (x \wedge \grad)^2 } - (x \wedge \grad) \wedge (x \wedge \grad) \\
&= (x \wedge \grad)^2 - (n-2) (x \wedge \grad) - (x \wedge \grad) \wedge (x \wedge \grad) \\
&= (-(n-2) + (x \wedge \grad) - (x \wedge \grad) \wedge ) (x \wedge \grad)
\end{aligned}
\end{equation}

The complete Laplacian is

\begin{equation}\label{eqn:bivectorSelect:goo12}
\begin{aligned}
\grad^2 &= \inv{x^2} (x \cdot \grad)^2 + (n - 2) \inv{x} \cdot \grad
- \inv{x^2}
\left(
(x \wedge \grad)^2 - (n-2) (x \wedge \grad) - (x \wedge \grad) \wedge (x \wedge \grad)
\right)
\end{aligned}
\end{equation}

In particular in less than four dimensions the quad-vector term is necessarily zero.  The 3D Laplacian becomes

\begin{equation}\label{eqn:bivectorSelect:goo13}
\begin{aligned}
\spacegrad^2 &= \inv{\Bx^2} (1 + \Bx \cdot \spacegrad)(\Bx \cdot \spacegrad)
+ \inv{\Bx^2} (1 - \Bx \wedge \spacegrad) (\Bx \wedge \spacegrad)
\end{aligned}
\end{equation}

So any eigenfunction of the bivector angular momentum operator \(\Bx \wedge \spacegrad\) is necessarily a simultaneous eigenfunction of the scalar operator.

%%\EndArticle
%\EndNoBibArticle
